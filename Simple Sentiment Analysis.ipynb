{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a text sentiment analysis model using Pytorch and torchtext on the IMDb dataset of movie reviews. For this notebook we will be using the RNN architecture to predict the sentiment inside a text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN Architecture:\n",
    "1. An RNN takes an input and performs an operation on the input along with a hidden state and then this output acts as an input to the next hidden state of the model along with an input.\n",
    "2. Here, the RNN model will take N inputs(Number of words in a sequence) and fuse them with a hidden state to produce an output. The output of the current hidden state will act as input for the next hidden state along with the next input in the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Screenshot%202023-04-22%20at%201.13.40%20PM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing Data:\n",
    "A central part of torchtext is Field. Field specifies how do you want your data to be processed. For our sentiment analysis task, the IMDB dataset consists of both the raw string - i.e. the movie review and also the sentiment for that statement - positive or negative. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding the technicalities:\n",
    "1. The parameters of FIELD define how you want your data to be processed.\n",
    "2. For our dataset-\n",
    "    a) TEXT defines how our raw string will be processed.\n",
    "    b) LABEL defines how to process the sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization - it is the task of splittng data into discrete 'tokens' so that we can work with textual data.\n",
    "TEXT field has an argument 'spacy', which indicates that we are using the spacy tokenizer.\n",
    "LABEL is used for handling the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /Users/adityajadhav/anaconda3/lib/python3.10/site-packages (3.5.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/adityajadhav/anaconda3/lib/python3.10/site-packages (from spacy) (22.0)\n",
      "Requirement already satisfied: jinja2 in /Users/adityajadhav/anaconda3/lib/python3.10/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /Users/adityajadhav/anaconda3/lib/python3.10/site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /Users/adityajadhav/anaconda3/lib/python3.10/site-packages (from spacy) (8.1.9)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/adityajadhav/anaconda3/lib/python3.10/site-packages (from spacy) (4.64.1)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /Users/adityajadhav/anaconda3/lib/python3.10/site-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/adityajadhav/anaconda3/lib/python3.10/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/adityajadhav/anaconda3/lib/python3.10/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/adityajadhav/anaconda3/lib/python3.10/site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/adityajadhav/anaconda3/lib/python3.10/site-packages (from spacy) (1.23.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/adityajadhav/anaconda3/lib/python3.10/site-packages (from spacy) (1.1.1)\n",
      "Requirement already satisfied: setuptools in /Users/adityajadhav/anaconda3/lib/python3.10/site-packages (from spacy) (65.6.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/adityajadhav/anaconda3/lib/python3.10/site-packages (from spacy) (2.28.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/adityajadhav/anaconda3/lib/python3.10/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/adityajadhav/anaconda3/lib/python3.10/site-packages (from spacy) (2.4.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/adityajadhav/anaconda3/lib/python3.10/site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/adityajadhav/anaconda3/lib/python3.10/site-packages (from spacy) (5.2.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /Users/adityajadhav/anaconda3/lib/python3.10/site-packages (from spacy) (1.10.7)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/adityajadhav/anaconda3/lib/python3.10/site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/adityajadhav/anaconda3/lib/python3.10/site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/adityajadhav/anaconda3/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.4.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/adityajadhav/anaconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/adityajadhav/anaconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/adityajadhav/anaconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/adityajadhav/anaconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/adityajadhav/anaconda3/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/adityajadhav/anaconda3/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/adityajadhav/anaconda3/lib/python3.10/site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/adityajadhav/anaconda3/lib/python3.10/site-packages (from jinja2->spacy) (2.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /Users/adityajadhav/anaconda3/lib/python3.10/site-packages (from en-core-web-sm==3.5.0) (3.5.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /Users/adityajadhav/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.7)\n",
      "Requirement already satisfied: setuptools in /Users/adityajadhav/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (65.6.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/adityajadhav/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/adityajadhav/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/adityajadhav/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.64.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/adityajadhav/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/adityajadhav/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (22.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/adityajadhav/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/adityajadhav/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (5.2.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/adityajadhav/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /Users/adityajadhav/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /Users/adityajadhav/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/adityajadhav/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: jinja2 in /Users/adityajadhav/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/adityajadhav/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/adityajadhav/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.28.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/adityajadhav/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /Users/adityajadhav/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.9)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/adityajadhav/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.23.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/adityajadhav/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/adityajadhav/anaconda3/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.4.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/adityajadhav/anaconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/adityajadhav/anaconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/adityajadhav/anaconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/adityajadhav/anaconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2022.12.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/adityajadhav/anaconda3/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/adityajadhav/anaconda3/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/adityajadhav/anaconda3/lib/python3.10/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/adityajadhav/anaconda3/lib/python3.10/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "from torchtext.vocab import Vocab\n",
    "\n",
    "#Set a random seed\n",
    "SEED=42 \n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "#Handling text data and labels\n",
    "TEXT = data.Field(tokenize = 'spacy',tokenizer_language = 'en_core_web_sm')\n",
    "LABEL = data.LabelField(dtype = torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the Dataset - \n",
    "The IMDb dataset is already pre-loaded in pytorch so we can directly import it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import IMDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the data into train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = IMDB.splits(TEXT, LABEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we find the number of data samples in both training and test datasets;\n",
    "The original dataset was structured to have 25000 training and 25000 test datasets so we should have 25000 samples in each split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training dataset :  25000\n",
      "Number of examples in test dataset :  25000\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of training dataset : \", len(train_data))\n",
    "print(\"Number of examples in test dataset : \", len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us view an example from our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['A', 'typical', 'romp', 'through', 'Cheech', 'and', 'Chong', \"'s\", 'reality', 'which', 'includes', 'drugs', ',', 'singing', ',', 'more', 'drugs', ',', 'cars', 'and', 'driving', ',', 'even', 'more', 'drugs', ',', 'Pee', 'Wee', ',', 'aliens', ',', 'gasoline', ',', 'laundry', ',', 'stand', 'up', 'comedy', ',', 'surprisingly', 'more', 'drugs', 'and', 'SPACE', 'COKE', '!', '!', '.', 'It', 'is', 'not', 'as', 'coherent', 'or', 'plausible', 'as', 'Up', 'in', 'Smoke', 'but', 'it', 'still', 'is', 'incredibly', 'funny', ',', 'without', 'becoming', 'as', 'strange', 'as', 'Nice', 'Dreams', '.', 'There', 'are', 'some', 'classic', 'scenes', ',', 'which', 'include', 'the', 'opening', 'scene', 'where', 'they', 'get', 'some', 'gas', 'for', 'their', 'car', 'and', 'the', 'drive', 'to', 'work', '.', 'Also', 'funny', 'is', 'Cheech', \"'s\", 'song', '(', 'Mexican', '-', 'Americans', ')', 'and', 'Chong', \"'s\", 'follow', 'up', 'song', '.', 'Another', 'notable', 'scene', 'is', 'the', 'welfare', 'office', 'scene', 'with', 'Jones', '(', 'human', 'noise', 'machine', ')', ',', 'from', 'the', 'Police', 'Academy', 'series', ',', 'and', 'the', 'old', 'laughing', 'man', '.', 'All', 'in', 'all', ',', 'this', 'is', 'a', 'great', 'follow', 'up', 'to', 'Up', 'in', 'Smoke', 'and', 'is', 'quite', 'watchable', 'when', 'sober', 'or', 'not.<br', '/><br', '/>-Celluloid', 'Rehab'], 'label': 'pos'}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data.examples[17]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to create a validation data set to gain an insight on how our model will perform on unseen data. With these insights, we can optimize our hyperparameters to fine tune and better fit our model. Use a validation dataset helps our model perform better on unseen data.\n",
    "\n",
    "Split using the random fucntion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "train_data, valid_data = train_data.split(random_state=random.seed(SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training dataset :  17500\n",
      "Number of examples in Validation dataset :  7500\n",
      "Number of examples in test dataset :  25000\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of training dataset : \", len(train_data))\n",
    "print(\"Number of examples in Validation dataset : \", len(valid_data))\n",
    "print(\"Number of examples in test dataset : \", len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabulary -\n",
    "\n",
    "For our text dataset, we have a composition of strings, we want to convert the string data into numbers and vectors for our model to work with.\n",
    "Vocabulary for our dataset is a collection of unique words in our dataset and then we encode these words using One Hot Encoding so that they are represented in a vector of 1*N where N is the number of unique words. Every unique word has a specific index assigned to it. The one hot vector will have 0's in the entire vector except at the index of the word where it takes a value of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train_data)\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in TEXT vocabulary: 101094\n",
      "Unique tokens in LABEL vocabulary: 2\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
    "print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are more than 100,000 unique words in our dataset. Creating a huge vector will be computationally very expensive and not feasible.\n",
    "So, we will limit our vocab size to only 25,000 tokens and we will have a vector of 25000 by 1.\n",
    "For the labels wwe have a categorical values which is why it has only two entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 25000\n",
    "\n",
    "TEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE)\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in TEXT vocabulary: 25002\n",
      "Unique tokens in LABEL vocabulary: 2\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
    "print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe two extra tokens from our max_size.\n",
    "These tokens are \n",
    "1.  'UNK' token for words not present in the vocab\n",
    "2.  'pad' token\n",
    "\n",
    "When we feed sentences to our model, we want all our sentences to be of the same length. This is however not true for all sentences so we add padding to the smaller ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to make our dataset iterable for which we use the BucketIterator. We want to iterate over them in a training loop.\n",
    "\n",
    "We'll use a BucketIterator which is a special type of iterator that will return a batch of examples where each example is of a similar length, minimizing the amount of padding per example.\n",
    "\n",
    "We want the tensors returned by the iterator on the GPU which is handled using torch.device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BUILDING THE MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this analysis task, we want to use a RNN Model.\n",
    "The embedding layer is used to transform our sparse one-hot vector (sparse as most of the elements are 0) into a dense embedding vector (dense as the dimensionality is a lot smaller and all the elements are real numbers). \n",
    "This embedding layer is simply a single fully connected layer.\n",
    "As well as reducing the dimensionality of the input to the RNN, there is the theory that words which have similar impact on the sentiment of the review are mapped close together in this dense vector space. \n",
    "\n",
    "So we want to capture the probabilities of one word occuring in context of the other words so we can capture semantic and syntactic meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Screenshot%202023-04-22%20at%204.36.26%20PM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The forward method is called when we feed our examples into our model.\n",
    "2. Each batch, TEXT is a tensor of size [sentencelength, batchsize](we have same sentence length for all our batches)\n",
    "3. This input batch will be passed to an embedding layer where it will be converted into a dense vector of co-dependent words.\n",
    "4. The output of the embedding layer acts as input to the RNN model.\n",
    "5. The RNN returns 2 tensors, output of size [sentence length, batch size, hidden dim] and hidden of size [1, batch size, hidden dim]. output is the concatenation of the hidden state from every time step, whereas hidden is simply the final hidden state. We verify this using the assert statement. Note the squeeze method, which is used to remove a dimension of size 1.\n",
    "6. Finally, we feed the last hidden state, hidden, through the linear layer, fc, to produce a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
    "        super (RNN, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, text):\n",
    "\n",
    "        #text = [sent len, batch size]\n",
    "        \n",
    "        embedded = self.embedding(text)\n",
    "        \n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "        \n",
    "        output, hidden = self.rnn(embedded)\n",
    "        \n",
    "        #output = [sent len, batch size, hid dim]\n",
    "        #hidden = [1, batch size, hid dim]\n",
    "        \n",
    "        assert torch.equal(output[-1,:,:], hidden.squeeze(0))\n",
    "        return self.fc(hidden.squeeze(0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We now create an instance of RNN class\n",
    "2. The input dimension is the size of vocab [25002 by 1]\n",
    "3. The embedding layer takes an input and then creates a dense vector.\n",
    "4. The output of the model is usually the number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "\n",
    "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAIN THE MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use stochastic gradient descent (SGD) to update the parameters of the module. The first argument is the parameters will be updated by the optimizer, the second is the learning rate, i.e. how much we'll change the parameters by when we do a parameter update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will define a loss function which will measure our loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to write a function that defines the accuracy of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train function iterates over the data one batch at a time.\n",
    "\n",
    "1. For each batch we zero the gradients. Each parameter in a model has a grad attribute which stores the gradient calculate by the criterion.\n",
    "2. After this we feed the batch to our model for which the model calculates gradients and loss for every batch, with the loss being averaged over all examples in the batch.\n",
    "3. We calculate the gradient of each parameter with loss.backward(), and then update the parameters using the gradients and optimizer algorithm with optimizer.step().\n",
    "4. The loss and accuracy is accumulated across the epoch, the .item() method is used to extract a scalar from a tensor which only contains a single value.\n",
    "5. Finally, we return the loss and accuracy, averaged across the epoch. The len of an iterator is the number of batches in the iterator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for batch in iterator:\n",
    "        #We need to zero the gradients before every iteration or they will be carried over to every iteration\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #We make a new prediction based on our model architecture\n",
    "        predictions = model(batch.text).squeeze(1)\n",
    "\n",
    "        #We define a loss functions which tells us misclasssified examples\n",
    "        loss = criterion(predictions, batch.label)\n",
    "\n",
    "        accuracy = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "        #This is the back prop algorithm, taking gradient wrt each parameter\n",
    "        loss.backward()\n",
    "\n",
    "        #Taking a step in the optimal direction\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += accuracy.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.label)\n",
    "            \n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will tell us the time taken between each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 5m 29s\n",
      "\tTrain Loss: 0.693 | Train Acc: 49.90%\n",
      "\t Val. Loss: 0.695 |  Val. Acc: 49.43%\n",
      "Epoch: 02 | Epoch Time: 5m 37s\n",
      "\tTrain Loss: 0.693 | Train Acc: 49.55%\n",
      "\t Val. Loss: 0.695 |  Val. Acc: 48.68%\n",
      "Epoch: 03 | Epoch Time: 5m 25s\n",
      "\tTrain Loss: 0.693 | Train Acc: 49.82%\n",
      "\t Val. Loss: 0.695 |  Val. Acc: 49.16%\n",
      "Epoch: 04 | Epoch Time: 5m 29s\n",
      "\tTrain Loss: 0.693 | Train Acc: 49.70%\n",
      "\t Val. Loss: 0.695 |  Val. Acc: 49.38%\n",
      "Epoch: 05 | Epoch Time: 5m 29s\n",
      "\tTrain Loss: 0.693 | Train Acc: 49.70%\n",
      "\t Val. Loss: 0.695 |  Val. Acc: 49.50%\n",
      "Epoch: 06 | Epoch Time: 66m 54s\n",
      "\tTrain Loss: 0.693 | Train Acc: 50.13%\n",
      "\t Val. Loss: 0.695 |  Val. Acc: 48.61%\n",
      "Epoch: 07 | Epoch Time: 5m 26s\n",
      "\tTrain Loss: 0.693 | Train Acc: 49.38%\n",
      "\t Val. Loss: 0.695 |  Val. Acc: 48.61%\n",
      "Epoch: 08 | Epoch Time: 5m 25s\n",
      "\tTrain Loss: 0.693 | Train Acc: 49.60%\n",
      "\t Val. Loss: 0.695 |  Val. Acc: 49.44%\n",
      "Epoch: 09 | Epoch Time: 6m 0s\n",
      "\tTrain Loss: 0.693 | Train Acc: 49.36%\n",
      "\t Val. Loss: 0.695 |  Val. Acc: 48.37%\n",
      "Epoch: 10 | Epoch Time: 5m 51s\n",
      "\tTrain Loss: 0.693 | Train Acc: 49.73%\n",
      "\t Val. Loss: 0.695 |  Val. Acc: 48.49%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.703 | Test Acc: 43.85%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('tut1-model.pt'))\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that RNNs don't do particularly well on this task because of the sentence has many token.\n",
    "1. Vanishing and Exploding Gradients - Finding derivative of a term in the beginining wrt the loss fucntion involves many terms in the chain rule. If one of these terms are near to zero the gradient is zero and learining is very slow\n",
    "2. Computational complexity - since it carries over memory from lot of states, it has high computational complexity.\n",
    "3. Lack Of Parallelism - RNNs are inherently sequential, which makes it difficult to parallelize the computation. This can limit the speed and scalability of the network.\n",
    "4. Difficulty In Capturing Long-Term Dependencies - Although RNNs are designed to capture information about past inputs, they can struggle to capture long-term dependencies in the input sequence. This is because the gradients can become very small as they propagate through time, which can cause the network to forget important information."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7352900849d3d20ec92be303988eadb1aceaf8784fba72e38c961fcf8cfa691f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
