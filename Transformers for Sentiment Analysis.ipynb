{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformers for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTING THE LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformer has already been trained with a specific vocabulary, which means we need to train with the exact same vocabulary and also tokenize our data in the same way that the transformer did when it was initially trained.\n",
    "\n",
    "The transformers library has tokenizers for each of the transformer models provided. In this case we are using the BERT model which ignores casing (i.e. will lower case every word). We get this by loading the pre-trained bert-base-uncased tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to find the number of tokens in our vocabulary for which will be using the tokenizer.vocab function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the tokenizer is as simple as calling tokenizer.tokenize on a string. This will tokenize and lower case the data in a way that is consistent with the pre-trained transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'world', 'how', 'are', 'you', '?']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize('Hello WORLD how ARE yoU?')\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be creating numerical indexes of the word which was already trained on our voabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7592, 2088, 2129, 2024, 2017, 1029]\n"
     ]
    }
   ],
   "source": [
    "indexes = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] [SEP] [PAD] [UNK]\n"
     ]
    }
   ],
   "source": [
    "init_token = tokenizer.cls_token\n",
    "eos_token = tokenizer.sep_token\n",
    "pad_token = tokenizer.pad_token\n",
    "unk_token = tokenizer.unk_token\n",
    "\n",
    "print(init_token, eos_token, pad_token, unk_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101 102 0 100\n"
     ]
    }
   ],
   "source": [
    "init_token_idx = tokenizer.convert_tokens_to_ids(init_token)\n",
    "eos_token_idx = tokenizer.convert_tokens_to_ids(eos_token)\n",
    "pad_token_idx = tokenizer.convert_tokens_to_ids(pad_token)\n",
    "unk_token_idx = tokenizer.convert_tokens_to_ids(unk_token)\n",
    "\n",
    "print(init_token_idx, eos_token_idx, pad_token_idx, unk_token_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "max_input_length = tokenizer.max_model_input_sizes['bert-base-uncased']\n",
    "print(max_input_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To explicitly get them from the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101 102 0 100\n"
     ]
    }
   ],
   "source": [
    "init_token_idx = tokenizer.cls_token_id\n",
    "eos_token_idx = tokenizer.sep_token_id\n",
    "pad_token_idx = tokenizer.pad_token_id\n",
    "unk_token_idx = tokenizer.unk_token_id\n",
    "\n",
    "print(init_token_idx, eos_token_idx, pad_token_idx, unk_token_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that the maximum length of the imput sequence is 512 tokens.\n",
    "\n",
    "For the RNN and CNN architectures, we used the spacy word tokenizer.\n",
    "Here, however, we will perform tokenization using a fucntion. A special edge case to consider that the maximum tokens in a sentence should be 510 tokens and 512 because we will be appending two extra tokens which will be the \"bos\" and \"eos\" token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_cut(sentence):\n",
    "  tokens = tokenizer.tokenize(sentence)\n",
    "  tokens = tokens[:max_input_length-2]\n",
    "  return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data\n",
    "\n",
    "TEXT = data.Field(batch_first = True,\n",
    "                  use_vocab = False,\n",
    "                  tokenize = tokenize_and_cut,\n",
    "                  preprocessing = tokenizer.convert_tokens_to_ids,\n",
    "                  init_token = init_token_idx,\n",
    "                  eos_token = eos_token_idx,\n",
    "                  pad_token = pad_token_idx,\n",
    "                  unk_token = unk_token_idx)\n",
    "\n",
    "LABEL = data.LabelField(dtype = torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOAD THE DATA AND CREATE TEST AND VALIDATION SPLITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import datasets\n",
    "\n",
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
    "train_data, valid_data = train_data.split(random_state = random.seed(SEED))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PRINTING THE NUMBER OF EXAMPLES IN EACH SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 17500\n",
      "Number of validation examples: 7500\n",
      "Number of testing examples: 25000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training examples: {len(train_data)}\")\n",
    "print(f\"Number of validation examples: {len(valid_data)}\")\n",
    "print(f\"Number of testing examples: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PRINT A TEST EXAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': [8040, 9541, 3762, 20160, 2003, 17319, 2028, 1997, 1996, 2087, 3722, 1010, 3144, 1998, 11419, 9476, 3494, 1999, 1996, 2088, 1012, 2061, 1010, 2054, 6433, 2043, 2017, 1005, 2310, 2042, 7249, 1998, 2589, 2673, 2007, 1996, 5675, 1029, 2017, 6942, 2009, 2039, 2157, 1029, 3308, 1012, 2017, 2644, 2537, 1998, 2292, 2009, 2717, 2005, 1037, 5476, 2030, 2061, 1998, 2059, 2448, 2009, 2153, 1010, 4363, 1996, 4563, 1997, 2049, 3112, 10109, 1012, 2008, 2003, 2000, 2360, 1010, 6293, 2007, 1996, 5675, 2005, 1996, 2087, 2112, 2021, 5587, 2115, 3327, 28126, 2000, 2009, 1012, 2023, 2000, 2033, 2003, 2339, 1000, 2054, 1005, 1055, 2047, 8040, 9541, 3762, 20160, 1000, 2499, 1010, 2027, 2215, 2067, 2000, 1996, 4438, 8040, 9541, 3762, 20160, 5675, 2029, 2018, 2069, 5147, 24501, 3126, 12172, 2094, 1037, 5476, 3041, 1999, 1000, 1037, 26781, 2315, 8040, 9541, 3762, 20160, 1000, 2021, 2005, 1996, 2087, 2112, 2018, 2025, 2042, 10410, 2144, 1996, 2434, 1000, 8040, 9541, 3762, 20160, 2073, 2024, 2017, 1000, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1996, 2034, 3696, 1006, 2000, 2033, 1007, 1997, 1037, 5410, 5378, 2003, 1996, 10502, 1997, 4469, 23585, 2271, 3494, 1025, 2045, 2453, 2022, 1037, 2261, 13545, 5758, 2013, 2627, 27758, 2015, 2021, 3227, 2065, 2017, 2228, 1000, 8040, 9541, 3762, 20160, 1000, 2017, 4995, 1005, 1056, 3241, 1997, 2143, 1011, 13109, 3286, 1010, 15121, 7685, 20160, 2030, 8040, 9541, 3762, 4241, 2213, 1012, 2130, 4788, 1010, 1996, 15945, 1997, 1996, 2060, 4563, 2372, 1997, 1000, 6547, 4297, 1000, 3227, 5769, 1037, 2177, 1997, 2537, 2111, 2040, 2123, 1005, 1056, 3305, 2013, 1037, 4268, 2391, 1997, 3193, 2129, 1996, 2265, 2573, 1012, 1996, 3937, 18458, 2038, 2467, 2042, 1037, 2177, 1997, 2111, 2040, 2024, 22939, 12589, 3973, 4941, 2893, 2362, 1998, 2083, 2037, 2219, 3265, 1010, 12991, 13874, 2094, 11647, 6133, 2000, 7505, 20048, 1996, 8518, 2445, 2012, 2192, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2023, 2279, 20423, 2003, 2074, 2026, 14833, 21885, 2075, 2061, 13558, 2009, 2065, 2017, 2215, 1024, 1045, 3246, 2008, 1045, 2064, 4863, 2339, 1045, 2228, 10882, 21814, 2105, 2007, 1996, 3937, 3787, 1997, 1996, 2265, 2024, 29172, 2007, 2026, 7613, 1997, 2054, 1996, 6080, 5836, 1998, 2129, 2027, 9002, 2000, 1996, 2878, 1025, 5965, 5836, 1996, 4062, 1010, 1045, 2228, 1999, 2236, 2009, 2003, 1996, 3800, 1997, 5965, 2000, 2507, 1996, 2177, 3257, 1010, 3029, 1998, 4942, 1011, 8518, 1012, 5965, 3475, 1005, 1056, 1037, 3407, 1011, 2175, 1011, 5341, 10563, 1010, 2002, 1005, 1055, 2115, 5795, 1010, 2115, 3836, 1010, 2115, 3611, 1010, 2115, 3691, 3275, 1012, 5965, 5829, 2302, 13431, 1998, 2003, 5533, 2011, 8518, 1006, 3291, 2467, 19635, 5576, 2005, 5965, 1007, 1012, 1999, 2116, 3971, 5965, 2003, 1996, 3424, 25078, 2000, 25741, 1012, 25741, 2003, 2115, 2190, 2767, 1010, 2008, 3124, 2040, 2003, 2074, 1037, 2210, 2062, 4452, 1997, 2477, 2084, 2017, 2024, 1010, 2002, 12939, 2017, 2000, 2022, 9191, 1010, 2000, 2025, 2022, 2012, 1996, 2067, 1997, 1996, 5308, 1012, 25741, 5836, 7603, 1998, 2003, 4703, 4760, 6832, 28800, 2013, 3449, 3370, 2000, 3571, 1012, 2310], 'label': 'neg'}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data.examples[6]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have stored this data in form of indices, so we need to get back the original tokens. \n",
    "We have stored the data in the form of a dictionary where we can easily fetch the token based upon its index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sc', '##oo', '##by', 'doo', 'is', 'undoubtedly', 'one', 'of', 'the', 'most', 'simple', ',', 'successful', 'and', 'beloved', 'cartoon', 'characters', 'in', 'the', 'world', '.', 'so', ',', 'what', 'happens', 'when', 'you', \"'\", 've', 'been', 'everywhere', 'and', 'done', 'everything', 'with', 'the', 'formula', '?', 'you', 'switch', 'it', 'up', 'right', '?', 'wrong', '.', 'you', 'stop', 'production', 'and', 'let', 'it', 'rest', 'for', 'a', 'decade', 'or', 'so', 'and', 'then', 'run', 'it', 'again', ',', 'keeping', 'the', 'core', 'of', 'its', 'success', 'intact', '.', 'that', 'is', 'to', 'say', ',', 'stick', 'with', 'the', 'formula', 'for', 'the', 'most', 'part', 'but', 'add', 'your', 'particular', 'flavour', 'to', 'it', '.', 'this', 'to', 'me', 'is', 'why', '\"', 'what', \"'\", 's', 'new', 'sc', '##oo', '##by', 'doo', '\"', 'worked', ',', 'they', 'want', 'back', 'to', 'the', 'classic', 'sc', '##oo', '##by', 'doo', 'formula', 'which', 'had', 'only', 'successfully', 'res', '##ur', '##face', '##d', 'a', 'decade', 'earlier', 'in', '\"', 'a', 'pup', 'named', 'sc', '##oo', '##by', 'doo', '\"', 'but', 'for', 'the', 'most', 'part', 'had', 'not', 'been', 'tapped', 'since', 'the', 'original', '\"', 'sc', '##oo', '##by', 'doo', 'where', 'are', 'you', '\"', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'the', 'first', 'sign', '(', 'to', 'me', ')', 'of', 'a', 'weak', 'offering', 'is', 'the', 'inclusion', 'of', 'extra', '##neo', '##us', 'characters', ';', 'there', 'might', 'be', 'a', 'few', 'fond', 'memories', 'from', 'past', 'iteration', '##s', 'but', 'generally', 'if', 'you', 'think', '\"', 'sc', '##oo', '##by', 'doo', '\"', 'you', 'aren', \"'\", 't', 'thinking', 'of', 'film', '-', 'fl', '##am', ',', 'scrap', '##py', 'doo', 'or', 'sc', '##oo', '##by', 'du', '##m', '.', 'even', 'worse', ',', 'the', 'exclusion', 'of', 'the', 'other', 'core', 'members', 'of', '\"', 'mystery', 'inc', '\"', 'generally', 'indicate', 'a', 'group', 'of', 'production', 'people', 'who', 'don', \"'\", 't', 'understand', 'from', 'a', 'kids', 'point', 'of', 'view', 'how', 'the', 'show', 'works', '.', 'the', 'basic', 'premise', 'has', 'always', 'been', 'a', 'group', 'of', 'people', 'who', 'are', 'dia', '##metric', '##ally', 'opposed', 'getting', 'together', 'and', 'through', 'their', 'own', 'individual', ',', 'stereo', '##type', '##d', 'qualities', 'manage', 'to', 'sur', '##mount', 'the', 'tasks', 'given', 'at', 'hand', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'this', 'next', 'paragraph', 'is', 'just', 'my', 'theo', '##riz', '##ing', 'so', 'skip', 'it', 'if', 'you', 'want', ':', 'i', 'hope', 'that', 'i', 'can', 'explain', 'why', 'i', 'think', 'fi', '##ddling', 'around', 'with', 'the', 'basic', 'elements', 'of', 'the', 'show', 'are', 'detrimental', 'with', 'my', 'interpretation', 'of', 'what', 'the', 'gang', 'represents', 'and', 'how', 'they', 'contribute', 'to', 'the', 'whole', ';', 'fred', 'represents', 'the', 'driver', ',', 'i', 'think', 'in', 'general', 'it', 'is', 'the', 'purpose', 'of', 'fred', 'to', 'give', 'the', 'group', 'direction', ',', 'organization', 'and', 'sub', '-', 'tasks', '.', 'fred', 'isn', \"'\", 't', 'a', 'happy', '-', 'go', '-', 'lucky', 'teenager', ',', 'he', \"'\", 's', 'your', 'boss', ',', 'your', 'teacher', ',', 'your', 'dad', ',', 'your', 'authority', 'figure', '.', 'fred', 'moves', 'without', 'hesitation', 'and', 'is', 'driven', 'by', 'tasks', '(', 'problem', 'always', 'equals', 'solution', 'for', 'fred', ')', '.', 'in', 'many', 'ways', 'fred', 'is', 'the', 'anti', '##thesis', 'to', 'shaggy', '.', 'shaggy', 'is', 'your', 'best', 'friend', ',', 'that', 'guy', 'who', 'is', 'just', 'a', 'little', 'more', 'afraid', 'of', 'things', 'than', 'you', 'are', ',', 'he', 'enables', 'you', 'to', 'be', 'brave', ',', 'to', 'not', 'be', 'at', 'the', 'back', 'of', 'the', 'pack', '.', 'shaggy', 'represents', 'emotion', 'and', 'is', 'frequently', 'showing', 'emotional', 'extremes', 'from', 'el', '##ation', 'to', 'fear', '.', 've']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(vars(train_data.examples[6])['text'])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a vocabulary for labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(None, {'neg': 0, 'pos': 1})\n"
     ]
    }
   ],
   "source": [
    "print(LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the previous models, we want to create iterators and batches to iterate over our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE, \n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BUILD THE MODEL:\n",
    "We will use the pre-trained version of the BERT model for training as we did earlier for the tokenizer. We are also using the vocabulary of the BERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "bert = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEFINING THE MODEL:\n",
    "\n",
    "1. In the previous approaches we were feeding the input vector to an embedding layer to get vector embeddings. Here we will be using the pre-trained Transformer model.\n",
    "2. These embeddings will then be fed into a GRU to produce a prediction for the sentiment of the input sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTGRUSentiment(nn.Module):\n",
    "    def __init__(self, bert,hidden_dim,output_dim,n_layers,bidirectional,dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.bert = bert\n",
    "\n",
    "        embedding_dim = bert.config.to_dict()['hidden_size']\n",
    "\n",
    "        self.rnn = nn.GRU(embedding_dim,\n",
    "                          hidden_dim,\n",
    "                          num_layers = n_layers,\n",
    "                          bidirectional = bidirectional,\n",
    "                          batch_first = True,\n",
    "                          dropout = 0 if n_layers < 2 else dropout) \n",
    "\n",
    "        self.out = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,text):\n",
    "\n",
    "        #text = [batch size, sent len]\n",
    "        with torch.no_grad():\n",
    "            embedded = self.bert(text)[0]\n",
    "\n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        _, hidden = self.rnn(embedded)\n",
    "\n",
    "        #hidden = [n layers * n directions, batch size, emb dim]\n",
    "        if self.rnn.bidirectional:\n",
    "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "        else:\n",
    "            hidden = self.dropout(hidden[-1,:,:])\n",
    "\n",
    "        #hidden = [batch size, hid dim]\n",
    "        output = self.out(hidden)\n",
    "        \n",
    "        #output = [batch size, out dim]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INITIALIZING HYPERPARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.25\n",
    "\n",
    "model = BERTGRUSentiment(bert,\n",
    "                         HIDDEN_DIM,\n",
    "                         OUTPUT_DIM,\n",
    "                         N_LAYERS,\n",
    "                         BIDIRECTIONAL,\n",
    "                         DROPOUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COUNTING THE NUMBER OF PARAMETERS IN OUR MODEL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 112,241,409 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to freeze paramers (not train them) we need to set their requires_grad attribute to False. To do this, we simply loop through all of the named_parameters in our model and if they're a part of the bert transformer model, we set requires_grad = False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():                \n",
    "    if name.startswith('bert'):\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAINING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CALCULATING THE ACCURACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(batch.text).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions, batch.label)\n",
    "        \n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.label)\n",
    "            \n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAINING THE TRASNFORMER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 5\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "        \n",
    "    end_time = time.time()\n",
    "        \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "        \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut6-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TESTING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('tut6-model.pt'))\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(model, tokenizer, sentence):\n",
    "    model.eval()\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    tokens = tokens[:max_input_length-2]\n",
    "    indexed = [init_token_idx] + tokenizer.convert_tokens_to_ids(tokens) + [eos_token_idx]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(0)\n",
    "    prediction = torch.sigmoid(model(tensor))\n",
    "    if prediction.item() >= 0.5:\n",
    "        print(\"Positive\")\n",
    "    else:\n",
    "        print(\"Negative\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_sentiment(model, tokenizer, \"This film is terrible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_sentiment(model, tokenizer, \"This film is great\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7352900849d3d20ec92be303988eadb1aceaf8784fba72e38c961fcf8cfa691f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
